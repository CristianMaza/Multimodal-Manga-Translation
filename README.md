# Automatic Manga Translation with Visual Context

This project explores the automatic translation of manga using multimodal language models, comparing unimodal and multimodal approaches with different levels of context.

## Objective

The main goal is to investigate how visual and textual context affects the quality of automatic manga translation, using zero-shot, one-shot, and few-shot learning techniques.

## Models Used

- **Unimodal (Translation)**: [Llama3 8B](https://ollama.com/library/llama3:8b)
- **Multimodal (Image Description)**: [LLaVA](https://ollama.com/library/llava)

Both models are run locally using Ollama.

## Repository Structure

- `notebook.ipynb`: Main Jupyter notebook containing all project code
- `open-mantra-dataset/`: Original dataset
- `open-mantra-dataset-frames/`: Modified dataset with manga pages separated into individual frames
- `frame_data_full.json`: JSON file containing full frame descriptions
- `frame_data_reduced.json`: JSON file containing reduced frame descriptions
- `frame_data_mini.json`: JSON file containing minimal frame descriptions

## Project Structure

The project is implemented in a single Jupyter notebook with the following structure:

1. Library imports
2. Dataset loading and preprocessing
   - JSON data loading
   - Text grouping (no context, frame-level, page-level)
3. Helper function definitions
4. Prompt implementations
   - Unimodal approach: Zero-shot, One-shot
   - Multimodal approach: 
     - Vision model (LLaVA 7B): Full description (Zero-shot), Reduced description (Few-shot)
     - Text model (Llama3 8B): Zero-shot, Few-shot, Chain-of-Thought
5. Experiments
   - Unimodal approach evaluation
   - Multimodal approach evaluation

## Frame Descriptions

The repository includes three JSON files containing frame descriptions generated by the LLaVA model:

1. `frame_data_full.json`: Contains detailed, comprehensive descriptions of each frame.
2. `frame_data_reduced.json`: Contains shorter, more focused descriptions of each frame.
3. `frame_data_mini.json`: Contains minimal, key-element descriptions of each frame.

These descriptions are used in the multimodal approach to provide visual context for the translation process.

## Setup and Usage

To run this project:

1. Clone the repository
2. Install the necessary dependencies:
   ```
   pip install -r requirements.txt
   ```
3. Install Ollama (https://ollama.ai/)
4. Download the Llama3 8B and LLaVA models using Ollama
5. Run the Jupyter notebook:
   ```
   jupyter notebook notebook.ipynb
   ```

### Ollama Configuration

Ensure Ollama is running and the models are available:

```
ollama run llama3:8b
ollama run llava
```

## Dataset

The project uses the Open-Mantra dataset, which is included in the repository. Two versions of the dataset are available:

1. `open-mantra-dataset/`: Original dataset
2. `open-mantra-dataset-frames/`: Modified dataset with pages separated into individual frames